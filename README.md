# itatech.IA
A intelig√™ncia artificial da empresa Itatech. Um modelo de chatbot ultraavan√ßado.

<p align="center">
<img width="1000px" alt="Itatech.IA" src="pictures/colocarImagem.png">
</p>
<p align="center"><a href="https://www.itatech.dev@gmail.com/">[<img src="pictures/colocarImagem.png" width="20px"> P√°gina inicial]</a> | <a href="https://itatech.dev@gmail.com/">[ü§ñ Bate-papo com o Itatech.Dev]</a> | <a href="https://linkDownloadDeModelos">[ü§ó Download de modelos]</a> | <a href="https://LinkDiscord">[Discord]</a> | <a href="https://github.com/Exemple`">[WeChat (ÂæÆ‰ø°)]</a></p>
<p align="center">
<a href="https://huggingface.co/papers/2401.14196"><b>Link do artigo</b>üëÅÔ∏è</a>
</p>
<hr>

### 1. Introdu√ß√£o ao DeepSeek Coder

O Itatech.IA √© composto por uma s√©rie de modelos de linguagem de c√≥digo, cada um treinado do zero em tokens 2T, com uma composi√ß√£o de 87% de c√≥digo e 13% de linguagem natural em ingl√™s e chin√™s. Fornecemos v√°rios tamanhos do modelo de c√≥digo, variando de vers√µes de 1B a 33B. Cada modelo √© pr√©-treinado no corpus de c√≥digo de n√≠vel de projeto, empregando um tamanho de janela de 16K e uma tarefa extra de preenchimento de lacunas, para dar suporte √† conclus√£o e preenchimento de c√≥digo de n√≠vel de projeto. Para recursos de codifica√ß√£o, o DeepSeek Coder atinge desempenho de ponta entre modelos de c√≥digo de c√≥digo aberto em v√°rias linguagens de programa√ß√£o e v√°rios benchmarks.

<p align="center">
<img src="pictures/result.png" alt="result" width="70%">
</p>

- **Dados de treinamento massivos**: treinados do zero em tokens 2T, incluindo 87% de c√≥digo e 13% de dados lingu√≠sticos em ingl√™s.

- **Altamente flex√≠vel e escal√°vel**: oferecido em tamanhos de modelo de 1B, 5,7B, 6,7B e 33B, permitindo que os usu√°rios escolham a configura√ß√£o mais adequada para seus requisitos.

- **Desempenho de modelo superior**: desempenho de √∫ltima gera√ß√£o entre modelos de c√≥digo dispon√≠veis publicamente nos benchmarks HumanEval, MultiPL-E, MBPP, DS-1000 e APPS.

- **Recursos avan√ßados de conclus√£o de c√≥digo**: um tamanho de janela de 16K e uma tarefa de preenchimento de lacunas, dando suporte √† conclus√£o de c√≥digo em n√≠vel de projeto e tarefas de preenchimento.

#### Linguagens de programa√ß√£o suportadas
`['ada', 'agda', 'alloy', 'antlr', 'applescript', 'assembly', 'augeas', 'awk', 'batchfile', 'bluespec', 'c', 'c-sharp', 'clojure', 'cmake', 'coffeescript', 'common-lisp', 'cpp', 'css', 'cuda', 'dart', 'dockerfile', 'elixir', 'elm', 'emacs-lisp', 'erlang', 'f-sharp', 'fortran', 'glsl', 'go', 'groovy', 'haskell', 'html', 'idris', 'isabelle', 'java', 'java-server-pages', 'javascript', 'json', 'julia', 'jupyter-notebook', 'kotlin', 'lean', 'literate-agda', 'literate-coffeescript', 'literate-haskell', 'lua', 'makefile', 'maple', 'markdown', 'mathematica', 'matlab', 'ocaml', 'pascal', 'perl', 'php', 'powershell', 'prolog', 'protocol-buffer', 'python', 'r', 'racket', 'restructuredtext', 'rmarkdown', 'ruby', 'rust', 'sas', 'scala', 'scheme', 'shell', 'smalltalk', 'solidity', 'sparql', 'sql', 'stan', 'standard-ml', 'stata', 'systemverilog', 'tcl', 'tcsh', 'tex', 'thrift', 'typescript', 'verilog', 'vhdl', 'visual-basic', 'xslt', 'yacc', 'yaml', 'zig']`

### 2. Resultados da avalia√ß√£o
Avaliamos o Itatech.IA em v√°rios benchmarks relacionados √† codifica√ß√£o.
Apenas os resultados `pass@1` no HumanEval (Python e Multilingual), MBPP e DS-1000 s√£o relatados aqui:

<p align="center">
<img src="pictures/table.png" alt="table" width="70%">
</p>

O resultado mostra que o Itatech.IA-Base-33B supera significativamente os LLMs de c√≥digo aberto existentes. Comparado com o CodeLlama-34B, ele lidera em 7,9%, 9,3%, 10,8% e 5,9%, respectivamente, no HumanEval Python, HumanEval Multilingual, MBPP e DS-1000.
Surpreendentemente, nosso Itatech.IA-Base-7B atinge o desempenho do CodeLlama-34B.
O modelo Itatech.IA-Instruct-33B ap√≥s o ajuste de instru√ß√µes supera o GPT35-turbo no HumanEval e obt√©m resultados compar√°veis ‚Äã‚Äãcom o GPT35-turbo no MBPP.

Mais detalhes da avalia√ß√£o podem ser encontrados na [Avalia√ß√£o detalhada](#6-detailed-evaluation-results).

### 3. Procedimento de cria√ß√£o de dados e treinamento do modelo

#### Cria√ß√£o de dados

- Etapa 1: coletar dados de c√≥digo do GitHub e aplicar as mesmas regras de filtragem que [StarCoder Data](https://github.com/bigcode-project/bigcode-dataset) para filtrar dados.
- Etapa 2: analisar as depend√™ncias de arquivos dentro do mesmo reposit√≥rio para reorganizar as posi√ß√µes dos arquivos com base em suas depend√™ncias.
- Etapa 3: Concatenar arquivos dependentes para formar um √∫nico exemplo e empregar minhash de n√≠vel de reposit√≥rio para desduplica√ß√£o.
- Etapa 4: Filtrar ainda mais o c√≥digo de baixa qualidade, como c√≥digos com erros de sintaxe ou baixa legibilidade.

<img src="pictures/data_clean.png" alt="data_creation" width="100%">

#### Treinamento do modelo

- Etapa 1: Inicialmente pr√©-treinado com um conjunto de dados consistindo de 87% de c√≥digo, 10% de linguagem relacionada ao c√≥digo (Github Markdown e StackExchange) e 3% de idioma chin√™s n√£o relacionado ao c√≥digo. Os modelos s√£o pr√©-treinados usando tokens de 1,8T e um tamanho de janela de 4K nesta etapa.
- Etapa 2: Pr√©-treinamento adicional usando um tamanho de janela estendido de 16K em 200B tokens adicionais, resultando em modelos fundamentais (**DeepSeek-Coder-Base**).
- Etapa 3: Ajuste fino de instru√ß√µes em 2B tokens de dados de instru√ß√µes, resultando em modelos ajustados por instru√ß√µes (**DeepSeek-Coder-Instruct**).

<img src="pictures/model_pretraining.png" alt="model_pretraining" width="100%">

### 4. Como usar
Antes de prosseguir, voc√™ precisar√° instalar as depend√™ncias necess√°rias. Voc√™ pode fazer isso executando o seguinte comando:
```
pip install -r requirements.txt

```
Uma demonstra√ß√£o tamb√©m est√° dispon√≠vel no [ü§ó Hugging Face Space](https://huggingface.co/spaces/itatech.IA-instruct), e voc√™ pode executar a demonstra√ß√£o localmente usando `app.py` na pasta [demo](https://github.com/itatech.IA/main/demo). (Obrigado a toda a equipe do HF pelo suporte)

Aqui est√£o alguns exemplos de como usar nosso modelo.

#### 1) Conclus√£o de c√≥digo
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
tokenizer = AutoTokenizer.from_pretrained("itatech.IA-6.7b-base", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("itatech.IA/-6.7b-base", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()
input_text = "#escreva um algoritmo de classifica√ß√£o r√°pida"
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_length=128)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```
Este c√≥digo produzir√° o seguinte resultado:
```
def quick_sort(arr):
if len(arr) <= 1:
return arr
pivot = arr[0]
left = []
right = []
for i in range(1, len(arr)):
if arr[i] < pivot:
left.append(arr[i])
else:
right.append(arr[i])
return quick_sort(left) + [pivot] + quick_sort(right)
```

#### 2) Inser√ß√£o de c√≥digo
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
tokenizer = AutoTokenizer.from_pretrained("itatech.IA-6.7b-base", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("itatech.IA-6.7b-base", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()
input_text = """<ÔΩúfim beginÔΩú>def quick_sort(arr):
if len(arr) <= 1:
return arr
pivot = arr[0]
left = []
right = []
<ÔΩúfim holeÔΩú>
if arr[i] < pivot:
left.append(arr[i])
else:
right.append(arr[i])
return quick_sort(left) + [pivot] + quick_sort(right)<ÔΩúfim endÔΩú>"""
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_length=128)
print(tokenizer.decode(outputs[0], skip_special_tokens=True)[len(input_text):])
```
Este c√≥digo produzir√° o seguinte resultado:
```
for i in range(1, len(arr)):
```

#### 3) Infer√™ncia do modelo de bate-papo
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
tokenizer = AutoTokenizer.from_pretrained("itatech.IA-6.7b-instruct", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("itatech.IA-6.7b-instruct", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()
messages=[
{ 'role': 'user', 'content': "escreva um algoritmo de classifica√ß√£o r√°pida em python."}
]
inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to(model.device)
# tokenizer.eos_token_id √© o id do token <|EOT|>
outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)
print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))
```
Este c√≥digo produzir√° o seguinte resultado:
```
Claro, aqui est√° uma implementa√ß√£o simples do algoritmo Quick Sort em Python:

def quick_sort(arr):
if len(arr) <= 1:
return arr
else:
pivot = arr[0]
less_than_pivot = [x for x in arr[1:] if x <= pivot]
greater_than_pivot = [x for x in arr[1:] if x > pivot]
return quick_sort(less_than_pivot) + [pivot] + quick_sort(greater_than_pivot)

# Teste a fun√ß√£o
arr = [10, 7, 8, 9, 1, 5]
print("Matriz original:", arr)
print("Matriz classificada:", quick_sort(arr))

Este c√≥digo funciona selecionando um elemento 'pivot' de o array e particionando os outros elementos em dois subarrays, de acordo com se eles s√£o menores ou maiores que o piv√¥. O elemento piv√¥ est√° ent√£o em sua posi√ß√£o final. O processo √© ent√£o repetido para os subarrays.
```

Se voc√™ n√£o quiser usar a API fornecida `apply_chat_template` que carrega o template de `tokenizer_config.json`, voc√™ pode usar o seguinte template para conversar com nosso modelo. Substitua o `['content']` por suas instru√ß√µes e as respostas anteriores do modelo (se houver), ent√£o o modelo ir√° gerar a resposta para a instru√ß√£o dada atualmente.
```
Voc√™ √© um assistente de programa√ß√£o de IA, utilizando o modelo DeepSeek Coder, desenvolvido pela itatech.IA Company, e voc√™ s√≥ responde perguntas relacionadas √† ci√™ncia da computa√ß√£o. Para perguntas politicamente sens√≠veis, quest√µes de seguran√ßa e privacidade, e outras quest√µes n√£o relacionadas √† ci√™ncia da computa√ß√£o, voc√™ se recusar√° a responder.
### Instru√ß√£o:
['content']
### Resposta:
['content']
<|EOT|>
### Instru√ß√£o:
['content']
### Resposta:

```

#### 4) Conclus√£o de c√≥digo em n√≠vel de reposit√≥rio
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/deepseek-coder-6.7b-base", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("deepseek-ai/deepseek-coder-6.7b-base", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()

input_text = """#utils.py
import tocha
de sklearn importar conjuntos de dados
de sklearn.model_selection importar train_test_split
de sklearn.preprocessing importar StandardScaler
de sklearn.metrics importar accuracy_score

def load_data():
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Padronizar os dados
scaler = StandardScaler()
X = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Converter dados numpy em tensores PyTorch
X_train = torch.tensor(X_train, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_train = tocha.tensor(y_train, dtype=torch.int64)
y_test = tocha.tensor(y_test, dtype=torch.int64)

return X_train, X_test, y_train, y_test

def avaliar_predi√ß√µes(y_test, y_pred):
return precis√£o_score(y_test, y_pred)

# model.py
importar tocha
importar tocha.nn como nn
importar tocha.optim como optim
de tocha.utils.data importar DataLoader, TensorDataset

classe IrisClassifier(nn.Module):
def __init__(self):
super(IrisClassifier, self).__init__()
self.fc = nn.Sequential(
nn.Linear(4, 16),
nn.ReLU(),
nn.Linear(16, 3)
)

def forward(self, x):
return self.fc(x)

def train_model(self, X_train, y_train, epochs, lr, batch_size):
criteria = nn.CrossEntropyLoss()
optimizer = optim.Adam(self.parameters(), lr=lr)

# Crie o DataLoader para lotes
dataset = TensorDataset(X_train, y_train)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

for epoch in range(epochs):
for batch_X, batch_y in dataloader:
optimizer.zero_grad()
outputs = self(batch_X)
loss = criteria(outputs, batch_y)
loss.backward()
optimizer.step()

def predict(self, X_test):
with torch.no_grad():
outputs = self(X_test)
_, predict = outputs.max(1)
return predict.numpy()

# main.py
from utils import load_data, assess_predictions
from model import IrisClassifier as Classifier

def main():
# Treinamento e avalia√ß√£o do modelo
"""
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=140)
print(tokenizer.decode(outputs[0]))
```

---
No cen√°rio a seguir, o modelo DeepSeek-Coder-6.7B efetivamente chama uma classe **IrisClassifier** e sua fun√ß√£o membro do arquivo `model.py` e tamb√©m utiliza fun√ß√µes do arquivo `utils.py` para complete corretamente a fun√ß√£o **main** no arquivo `main.py` para treinamento e avalia√ß√£o do modelo.

![GIF de conclus√£o](pictures/completion_demo.gif)

### 5. Como ajustar o DeepSeek-Coder

N√≥s fornecemos o script `finetune/finetune_deepseekcoder.py` para que os usu√°rios ajustem nossos modelos em tarefas posteriores.

O script suporta o treinamento com [DeepSpeed](https://github.com/microsoft/DeepSpeed). Voc√™ precisa instalar os pacotes necess√°rios por:

```bash
pip install -r finetune/requirements.txt
```

Siga o [Formato do conjunto de dados de amostra](https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1) para preparar seus dados de treinamento.
Cada linha √© uma string serializada em json com dois campos obrigat√≥rios `instruction` e `output`.

Ap√≥s a prepara√ß√£o dos dados, voc√™ pode usar o script de shell de exemplo para ajustar `deepseek-ai/deepseek-coder-6.7b-instruct`.
Lembre-se de especificar `DATA_PATH`, `OUTPUT_PATH`.
E escolha hiperpar√¢metros apropriados (por exemplo, `learning_rate`, `per_device_train_batch_size`) de acordo com seu cen√°rio.

```bash
DATA_PATH="<seu_caminho_de_dados>"
OUTPUT_PATH="<seu_caminho_de_sa√≠da>"
MODEL="itatech.IA-6.7b-instruct"

cd finetune && deepspeed finetune_itatechia.py \
--model_name_or_path $MODEL_PATH \
--data_path $DATA_PATH \
--output_dir $OUTPUT_PATH \
--num_train_epochs 3 \
--model_max_length 1024 \
--per_device_train_batch_size 16 \
--per_device_eval_batch_size 1 \
--gradient_accumulation_steps 4 \
--evaluation_strategy "no" \
--save_strategy "steps" \
--save_steps 100 \
--save_total_limit 100 \
--learning_rate 2e-5 \
--warmup_steps 10 \
--logging_steps 1 \
--lr_scheduler_type "cosseno" \
--gradient_checkpointing True \
--report_to "tensorboard" \
--deepspeed configs/ds_config_zero3.json \
--bf16 True
```

### 6. Resultados detalhados da avalia√ß√£o

O c√≥digo reproduz√≠vel para os seguintes resultados de avalia√ß√£o pode ser encontrado no diret√≥rio [Evaluation](https://github.com/itatech.IA/main/Evaluation).
#### 1) Benchmark HumanEval multil√≠ngue
![HumanEval](pictures/HumanEval.png)

#### 2) Benchmark MBPP
<img src="pictures/MBPP.png" alt="MBPP" width="40%">

#### 3) Benchmark DS-1000
![DS-1000](pictures/DS-1000.png)

#### 4) Benchmark de racioc√≠nio matem√°tico com aux√≠lio de programa
![Math](pictures/Math.png)

### Infer√™ncia com vLLM

Voc√™ tamb√©m pode empregar [vLLM](https://github.com/vllm-project/vllm) para infer√™ncia de alto rendimento.

**Completamento de texto**

```python
from vllm import LLM, SamplingParams

tp_size = 4 # Paralelismo de tensor
sampling_params = SamplingParams(temperature=0.7, top_p=0.9, max_tokens=100)
model_name = "deepseek-ai/deepseek-coder-6.7b-base"
llm = LLM(model=model_name, trust_remote_code=True, gpu_memory_utilization=0.9, tensor_parallel_size=tp_size)

prompts = [
"Se todos em um pa√≠s se amam,",
"A pesquisa tamb√©m deve se concentrar nas tecnologias",
"Para determinar se o r√≥tulo est√° correto, precisamos"
]
outputs = llm.generate(prompts, sampling_params)

generated_text = [output.outputs[0].text para sa√≠da em outputs]
print(generated_text)
```

**Conclus√£o do bate-papo**

```python
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

tp_size = 4 # Paralelismo de tensor
sampling_params = SamplingParams(temperatura=0,7, top_p=0,9, max_tokens=100)
model_name = "itatech.IA-6.7b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
llm = LLM(model=model_name, trust_remote_code=True, gpu_memory_utilization=0,9, tensor_parallel_size=tp_size)

messages_list = [
[{"role": "user", "content": "Quem √© voc√™?"}],
[{"role": "user", "content": "O que voc√™ pode fazer?"}],
[{"role": "user", "content": "Explique o Transformer brevemente."}],
]
prompts = [tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False) para mensagens em messages_list]

sampling_params.stop = [tokenizer.eos_token]
outputs = llm.generate(prompts, sampling_params)

generated_text = [output.outputs[0].text para sa√≠da em outputs]
print(generated_text)
```

### 7. Perguntas e respostas

#### Poderia Voc√™ fornece o arquivo tokenizer.model para quantiza√ß√£o do modelo?

O itatech.IA utiliza o [HuggingFace Tokenizer](https://huggingface.co/docs/tokenizers/index) para implementar o algoritmo Bytelevel-BPE, com pr√©-tokenizadores especialmente projetados para garantir o desempenho ideal. Atualmente, n√£o h√° uma maneira direta de converter o tokenizador em um tokenizador SentencePiece. Estamos contribuindo para que os m√©todos de quantiza√ß√£o de c√≥digo aberto facilitem o uso do HuggingFace Tokenizer.

##### GGUF(llama.cpp)

Enviamos um [PR](https://github.com/ggerganov/llama.cpp/pull/4070) para o popular reposit√≥rio de quantiza√ß√£o [llama.cpp](https://github.com/ggerganov/llama.cpp) para oferecer suporte total a todos os pr√©-tokenizadores HuggingFace, incluindo o nosso.

Enquanto espera que o PR seja mesclado, voc√™ pode gerar seu modelo GGUF usando as seguintes etapas:

```bash
git clone https://github.com/DOGEwbx/llama.cpp.git
cd llama.cpp
git checkout regex_gpt2_preprocess
# configure o ambiente de acordo com o README
make
python3 -m pip install -r requirements.txt
# gere o modelo GGUF
python convert-hf-to-gguf.py <MODEL_PATH> --outfile <GGUF_PATH> --model-name deepseekcoder
# use a quantiza√ß√£o q4_0 como exemplo
./quantize <GGUF_PATH> <OUTPUT_PATH> q4_0
./main -m <OUTPUT_PATH> -n 128 -p <PROMPT>
```
##### GPTQ(exllamav2)

`ATUALIZA√á√ÉO:`[exllamav2](https://github.com/turboderp/exllamav2) conseguiu suportar o Huggingface Tokenizer. Baixe a vers√£o mais recente e experimente.

Lembre-se de definir a escala RoPE para 4 para sa√≠da correta, mais discuss√£o pode ser encontrada neste [PR](https://github.com/turboderp/exllamav2/pull/189).

#### Como usar o deepseek-coder-instruct para completar o c√≥digo?

Embora os modelos deepseek-coder-instruct n√£o sejam especificamente treinados para tarefas de conclus√£o de c√≥digo durante o ajuste fino supervisionado (SFT), eles mant√™m a capacidade de executar a conclus√£o de c√≥digo de forma eficaz. Para habilitar essa funcionalidade, voc√™ simplesmente precisa ajustar o par√¢metro eos_token_id. Defina o eos_token_id como 32014, em oposi√ß√£o ao seu valor padr√£o de 32021 na configura√ß√£o itatech.IA-instruct. Essa modifica√ß√£o solicita que o modelo reconhe√ßa o fim de uma sequ√™ncia de forma diferente, facilitando assim as tarefas de conclus√£o de c√≥digo.

### 8. Recursos
[awesome-deepseek-coder](https://github.com/iatech.IA) √© uma lista com curadoria de projetos de c√≥digo aberto relacionados ao itatech.IA.

### 9. Licen√ßa
Este reposit√≥rio de c√≥digo √© licenciado sob a Licen√ßa MIT. O uso de modelos DeepSeek Coder est√° sujeito √† Licen√ßa de Modelo. DeepSeek Coder suporta uso comercial.

Consulte [C√ìDIGO DE LICEN√áA](C√ìDIGO DE LICEN√áA) e [MODELO DE LICEN√áA](MODELO DE LICEN√áA) para obter mais detalhes.

### 10. Cita√ß√£o
```
@misc{deepseek-codificador,
 autor = {Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y.K. Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang},
title = {DeepSeek-Coder: Quando o modelo de linguagem grande encontra a programa√ß√£o -- A ascens√£o da intelig√™ncia de c√≥digo},
journal = {CoRR},
volume = {abs/2401.14196},
year = {2024},
url = {https://arxiv.org/abs/2401.14196},
}
```

### 11. Contato

Se voc√™ tiver alguma d√∫vida, abra um problema ou entre em contato conosco em [itatech.dev@gmail.com](mailto:service@itatech.dev.com).
